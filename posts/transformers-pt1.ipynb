{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Building a Transformer-based Translator: Part 1\"\n",
    "author: \"Dominic Leon Culver\"\n",
    "date: \"2024-10-14\"\n",
    "draft: true\n",
    "categories: [transformers, machine-learning, deep-learning, model-implementation]\n",
    "bibliography: references.bib\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello There! Welcome to my blog! This is my first post, and I am excited to share it with you. In this series, I'll be walking you through a project where I built and trained a **transformer-based translator** from scratch.  It was a challenging (and occasionally very frustrating) project, but also highly rewarding and I learned a ton from doing it.\n",
    "\n",
    "I tackled this project for a copy of reasons. \n",
    "\n",
    "1. **Deepening my understanding of Transformers**. I've used them in other projects (e.g. sentiment analysis with BERT), but I wanted to take a closer look under the hood and really understand how they work internally. \n",
    "2. **Improving my PyTorch skills**. I've built models in PyTorch before and loved its flexibility, but I wanted to push my skills further by building something more complex from scratch. \n",
    "3. **Aspiring to do research**. I am very interested in the current research landscape surronding LLMs, and I'd love to eventually contribute to this area. In order to do that, a strong understanding of transformers and how they work is essential. \n",
    "\n",
    "When I initially started this project, I thought it would be fairly straightforward. However, that is *not* what happened, and because of that I will be breaking this series into three parts. In this post, I will cover more of the history and the ideas around machine translation in general and discuss the motivations and ideas behind the attention mechanism. In the next posts, I will dive into the specifics in the original *Attention is All You Need* [@Vaswani:2017aa] paper and walk through how to turn that into actual code. After that, I will discuss the training process, model evaluation, and what I'd do differently if I tackled this project again. I'll also talk about some of the services I used throughout the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's post we will delve into the context in which transformers appeared. In particular we will discuss the main ideas behind the transformers architecture before delving too deep into the actual code. Today's post will focus on \n",
    "\n",
    "1. How to regard language translation as a machine learning task. \n",
    "2. General approaches to machine translation architecture. \n",
    "3. Tokenization \n",
    "4. The advent of the ``attention mechanism''.\n",
    "5. Challenges with traditional machine translation architectures and the advantages of transformers\n",
    "\n",
    "By going through these points, I hope that you will be given sufficient context to understand what transformers are trying to do and why it garnered as much excitement as it did. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation as a Machine Learning Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before touching transformers, it is a natural question to ask why we should believe that translating a sentence from one language into another can even be solved using machine learning. Language is complicated and different languages often have vastly differing ways of expressing ideas between them. Take for example languages like English and Japanese. English generally uses the word order Subject-Verb-Object (SVO), i.e. \"The boy throws the ball\". The \"boy\" here is the subject, \"throws\" is the verb, and \"ball\" is the object. In Japanese, the word order is Subject-Object-Verb (SOV) instead. In Japanese, our sentence would instead be:\n",
    "\n",
    ">**Japanese Sentence**:  \n",
    ">少年はボールを投げる。  \n",
    ">\"Shōnen wa bōru o nageru\"  \n",
    ">**English Word-by-Word**:  \n",
    ">\"Boy ball throws\"\n",
    "\n",
    "As we can see from this example as well Japanese uses fewer articles than English. This is only a sampling of the vast differences that exist across all human languages. There are, as we've seen, issues related to linguistic differences (SVO vs SOV), but also cultural (idioms, jokes, or proverbs not translating well). Suffice it to say this makes modeling translation a difficult task. That being said, we can still make reasonable assumptions that can aid us in determining whether or not translation is something a machine learning algorithm can solve. \n",
    "\n",
    "First, we will assume that there is a source language `src_lang` and a target language `tgt_lang`, i.e. a language $X$ we hope to translate various sentences into language $Y$. As anyone who has learned another language can tell you, translation is not simply the task of taking a word and finding its corresponding word in the target language. Indeed, there is often not a direct mapping. For instance, in English the phrase \"to get\" can be used in several different ways, e.g. \"I am getting something from the store\" or \"I got a present\". In German, for instance, these would be translated with two separate verbs, \"holen\" in the first case and \"bekommen\" in the second. \n",
    "\n",
    "Despite these difficulties that exist in natural language, there is a long history in ML of using probabilities to predict the next word in a string. For example, \n",
    "\n",
    "> The sky was ...\n",
    "\n",
    "The next word could be \"blue\" or \"clear\" or \"cloudy\", but probably not \"wet\" or \"discrete\". We would say that \"blue\" is more _probable_ then \"discrete\". In NLP we often think of the task of next word prediction as approximating the probability \n",
    "$$\n",
    "    P(w_t | w_{t-1}\\cdots w_0)\n",
    "$$\n",
    "i.e. the probability of the next word appearing given the previous words. In fact, [@jm3] define a _language model_ as models which assign a probability to an entire sentence. From perusing [@jm3] one sees that many models that people have tried over the use essentially boil down to producing better and better algorithms to approximate these conditional probabilities. \n",
    "\n",
    "When dealing with the subject of translating from a source language to a target language, we can think of it in much the same way. We think of the source sentence as a sequence of words\n",
    "$$\n",
    "\\mathbf{x} = (x_0, \\ldots, x_S)\n",
    "$$\n",
    "and the target translation as a sequence of words\n",
    "$$\n",
    "\\mathbf{y} = (y_0, \\ldots, y_T).\n",
    "$$\n",
    "We can think of the translation task as a sequence of probability estimations, namely for $t$ between 0 and $T$ we wish to predict the next word by maximizing the following probability,\n",
    "$$\n",
    "P(y_t | y_{t-1}, \\ldots, y_0, x_S, \\ldots, x_0).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges with Traditional Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
