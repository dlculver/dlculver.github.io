[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "notes/notes.html",
    "href": "notes/notes.html",
    "title": "Notes",
    "section": "",
    "text": "Slides from a talk I gave on join work with J.D. Quigley on the kq-resolution over the field of complex numbers.\nNotes for a talk on modular forms I gave at European Talbot 2017."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dominic Culver’s Homepage",
    "section": "",
    "text": "Email Address: dominic period culver at gmail dot com\nWelcome to my homepage! Here you will find some information about me and some of the things I have worked on. If anything here piques your interest, please feel free to reach out!"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Dominic Culver’s Homepage",
    "section": "About Me",
    "text": "About Me\nI am a mathematician by training and I got my PhD in 2017 from the University of Notre Dame. I then went on to do postdocs first at the University of Illinois, Urbana-Champaign, and then another postdoc at the Max Planck Institute for Mathematics in Bonn Germany. My mathematical research interests were all within stable homotopy theory, and I was particularly interested in using ideas around elliptic curves and modular forms to understand complex structures in the stable homotopy groups of spheres.\nIn 2023, I decided to take the leap and transition into tech. I worked at an early legal AI startup for a year and half where I learned a lot of cool things about software and LLMs. My current interests are around ML and AI broadly, and NLP and LLMs more specifically. I am interested in building new and exciting things with these technologies while also using mathematical background to better understand LLMs in theory.\nI am currently looking for new opportunities which utilize my mathematical and research background and within AI/ML. I am also always happy to collaborate or discuss research ideas, please feel free to peruse my website and reach out!"
  },
  {
    "objectID": "research/research.html",
    "href": "research/research.html",
    "title": "Research and Publications",
    "section": "",
    "text": "In progress\n\nThe structure of the (v_2)-local algebraic tmf resolution (joint with Mark Behrens and Prasit Bhattacharya). Under Review\nWe give a complete description of the (E_1)-page of the (v_2)-local algebraic tmf resolution.\n\n\n\nPublications & Preprints\n\nAlgebraic K-theory of Elliptic Cohomology (with Gabe Angelini-Knoll, Christian Ausoni, Eva Höning, and John Rognes). Accepted, to appear in Geometry and Topology\nWe compute the (V(2))-homotopy of the algebraic K-theory of the second truncated Brown-Peterson spectrum.\nExotic K(h)-local Picard groups when 2p-1=h^2 and the Vanishing Conjecture (joint with Ningchuan Zhang). Accepted.\nThe Motivic Lambda algebra (joint with J.D. Quigley and W. Balderrama). Accepted, to appear in Geometry and Topology\nWe give a description of the motivic Lambda algebra over general base fields. We recover various calculations of Ext over the motivic Steenrod algebra and use these to determine an infinite family of (d_2)-differentials out of the 1-line in the R-motivic ASS.\nTopological Hochschild Homology of the second truncated Brown-Peterson spectrum, part 1 (Joint with Gabe Angelini-Knoll and Eva Höning)\nAlgebraic Slice Spectral Sequences (Joint with H. Kong and J.D. Quigley). Accepted. To appear in Documenta Mathematica\nThe telescope conjecture at height 2 and the tmf resolution (Joint with Behrens, Beaudry, Bhattacharya, and Xu). Accepted. To appear in the Journal of Topology\nWe analyze the tmf-based Adams spectral sequence for the type 2 spectrum Z. This is used to deduce that the K(2)-local Adams-Novikov spectral sequence, whose E2-term was computed by Bhattacharya-Egger, collapses. We also explore how this spectral sequence detects the telescopic homotopy of Z.\nK(1)-local tmf co-operations (Joint with Paul VanKoughnett). Accepted. To appear in JHRS\nUsing Hopkins’ construction of the spectrum tmf in the K(1)-local category, we compute the K(1)-local tmf cooperations.\nkq-resolutions I (Joint with JD Quigley). Accepted. To appear in Trans. AMS\nWe compute the cooperations of the very effective cover of Hermitian K-theory over the complex numbers. We use this to analyze the kq-resolution and prove complex motivic analogs of Mahowald’s results using bo-resolutions, such as the order of the 2-torsion in the image of J. We also formulate and provide evidence for a motivic Telescope Conjecture.\nThe Adams spectral sequence for 3-local tmf. Accepted. To appear in JHRS.\nLast updated 10/08/2020\nThe BP&lt;2&gt;-cooperations algebra at odd primes. JPAA 224 (May 2020)\nLast updated 9/20/19\nOn BP&lt;2&gt;-cooperations. Algebraic & Geometric Topology 19 (2019) 807-862\nOn the E2-term of the bo-Adams spectral sequence. J. Topology 13 (2020) 356-415.\n(with Mark Behrens, Agnes Beaudry, Prasit Bhattacharya, and Zhouli Xu). Last updated 2/1/17.\nA new basis for complex K-theory cooperations algebra. Topology and its Applications 234 (2018) 7-25"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "teaching/teaching.html",
    "href": "teaching/teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "I am not currently teaching."
  },
  {
    "objectID": "teaching/teaching.html#current-courses",
    "href": "teaching/teaching.html#current-courses",
    "title": "Teaching",
    "section": "",
    "text": "I am not currently teaching."
  },
  {
    "objectID": "teaching/teaching.html#past-courses",
    "href": "teaching/teaching.html#past-courses",
    "title": "Teaching",
    "section": "Past Courses:",
    "text": "Past Courses:\n\nSpring 2020: Math 595 - Chromatic Homotopy Theory\nFall 2019: Math 416 and 526\nSpring 2019: Math 527\nSpring 2019: Math 402\nFall 2018: Math 347\nSpring 2018: Math 402\nFall 2017: Math 347"
  },
  {
    "objectID": "posts/transformers-pt1.html",
    "href": "posts/transformers-pt1.html",
    "title": "Building a Transformer-based Translator: Part 1",
    "section": "",
    "text": "Hello There! Welcome to my blog! This is my first post, and I am excited to share it with you. In this series, I’ll be walking you through a project where I built and trained a transformer-based translator from scratch. It was a challenging (and occasionally very frustrating) project, but also highly rewarding and I learned a ton from doing it.\nI tackled this project for a copy of reasons.\n\nDeepening my understanding of Transformers. I’ve used them in other projects (e.g. sentiment analysis with BERT), but I wanted to take a closer look under the hood and really understand how they work internally.\nImproving my PyTorch skills. I’ve built models in PyTorch before and loved its flexibility, but I wanted to push my skills further by building something more complex from scratch.\nAspiring to do research. I am very interested in the current research landscape surronding LLMs, and I’d love to eventually contribute to this area. In order to do that, a strong understanding of transformers and how they work is essential.\n\nWhen I initially started this project, I thought it would be fairly straightforward. However, that is not what happened, and because of that I will be breaking this series into three parts. In this post, I will cover more of the history and the ideas around machine translation in general and discuss the motivations and ideas behind the attention mechanism. In the next posts, I will dive into the specifics in the original Attention is All You Need (Vaswani et al. 2017) paper and walk through how to turn that into actual code. After that, I will discuss the training process, model evaluation, and what I’d do differently if I tackled this project again. I’ll also talk about some of the services I used throughout the project.\n\nIntroduction\nIn today’s post we will delve into the context in which transformers appeared. In particular we will discuss the main ideas behind the transformers architecture before delving too deep into the actual code. Today’s post will focus on\n\nHow to regard language translation as a machine learning task.\nGeneral approaches to machine translation architecture.\nTokenization\nThe advent of the ``attention mechanism’’.\nChallenges with traditional machine translation architectures and the advantages of transformers\n\nBy going through these points, I hope that you will be given sufficient context to understand what transformers are trying to do and why it garnered as much excitement as it did.\n\n\nTranslation as a Machine Learning Task\nBefore touching transformers, it is a natural question to ask why we should believe that translating a sentence from one language into another can even be solved using machine learning. Language is complicated and different languages often have vastly differing ways of expressing ideas between them. Take for example languages like English and Japanese. English generally uses the word order Subject-Verb-Object (SVO), i.e. “The boy throws the ball”. The “boy” here is the subject, “throws” is the verb, and “ball” is the object. In Japanese, the word order is Subject-Object-Verb (SOV) instead. In Japanese, our sentence would instead be:\n\nJapanese Sentence:\n少年はボールを投げる。\n“Shōnen wa bōru o nageru”\nEnglish Word-by-Word:\n“Boy ball throws”\n\nAs we can see from this example as well Japanese uses fewer articles than English. This is only a sampling of the vast differences that exist across all human languages. There are, as we’ve seen, issues related to linguistic differences (SVO vs SOV), but also cultural (idioms, jokes, or proverbs not translating well). Suffice it to say this makes modeling translation a difficult task. That being said, we can still make reasonable assumptions that can aid us in determining whether or not translation is something a machine learning algorithm can solve.\nFirst, we will assume that there is a source language src_lang and a target language tgt_lang, i.e. a language \\(X\\) we hope to translate various sentences into language \\(Y\\). As anyone who has learned another language can tell you, translation is not simply the task of taking a word and finding its corresponding word in the target language. Indeed, there is often not a direct mapping. For instance, in English the phrase “to get” can be used in several different ways, e.g. “I am getting something from the store” or “I got a present”. In German, for instance, these would be translated with two separate verbs, “holen” in the first case and “bekommen” in the second.\nDespite these difficulties that exist in natural language, there is a long history in ML of using probabilities to predict the next word in a string. For example,\n\nThe sky was …\n\nThe next word could be “blue” or “clear” or “cloudy”, but probably not “wet” or “discrete”. We would say that “blue” is more probable then “discrete”. In NLP we often think of the task of next word prediction as approximating the probability \\[\n    P(w_t | w_{t-1}\\cdots w_0)\n\\] i.e. the probability of the next word appearing given the previous words. In fact, (Jurafsky and Martin 2024) define a language model as models which assign a probability to an entire sentence. From perusing (Jurafsky and Martin 2024) one sees that many models that people have tried over the use essentially boil down to producing better and better algorithms to approximate these conditional probabilities.\nWhen dealing with the subject of translating from a source language to a target language, we can think of it in much the same way. We think of the source sentence as a sequence of words \\[\n\\mathbf{x} = (x_0, \\ldots, x_S)\n\\] and the target translation as a sequence of words \\[\n\\mathbf{y} = (y_0, \\ldots, y_T).\n\\] We can think of the translation task as a sequence of probability estimations, namely for \\(t\\) between 0 and \\(T\\) we wish to predict the next word by maximizing the following probability, \\[\nP(y_t | y_{t-1}, \\ldots, y_0, x_S, \\ldots, x_0).\n\\]\n\n\nEncoder-Decoder Architecture\n\n\nTokenization\n\n\nThe Attention Mechanism\n\n\n\n\n\n\n\n\nChallenges with Traditional Architectures\n\nJurafsky, Daniel, and James H. Martin. 2024. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models. 3rd ed. https://web.stanford.edu/~jurafsky/slp3/.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need,” June. https://arxiv.org/pdf/1706.03762.pdf."
  }
]